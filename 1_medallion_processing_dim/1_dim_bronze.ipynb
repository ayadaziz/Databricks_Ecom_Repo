{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d231c00e-5c98-408c-b4e7-d88e7c0d0a22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, BooleanType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0597b2ee-add2-448a-94f1-768b3f98e698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98e19b88-f02b-4ee7-a471-10c1f8e4bdfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = 'ecommerce'\n",
    "\n",
    "#Define schema for the data file \n",
    "brand_schema = StructType([\n",
    "    StructField('brand_code', StringType(), False),\n",
    "    StructField('brand_name', StringType(), True),\n",
    "    StructField('category_code', StringType(), True),\n",
    "])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2546e14b-2a8b-421f-bd85-de5c42465c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_data_path = \"/Volumes/ecommerce/source_data/raw/brands/*.csv\"\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \",\").schema(brand_schema).csv(raw_data_path)\n",
    "    \n",
    "# add metadata columns \n",
    "df = df.withColumn(\"_source_file\",F.col(\"_metadata.file_path\"))\\\n",
    "       .withColumn(\"ingested_at\",F.current_timestamp())\n",
    "\n",
    "display(df.limit(5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71939a87-c32a-4339-8b5e-ac0ef50acb25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{catalog_name}.bronze.brz_brands\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57897d9d-614d-440e-923b-a392ad44c2eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b25cd50-b378-4a13-b157-ed273142f9b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = 'ecommerce'\n",
    "\n",
    "\n",
    "category_schema = StructType([\n",
    "    StructField(\"category_code\", StringType(), False),\n",
    "    StructField(\"category_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load data using the schema defined\n",
    "raw_data_path = \"/Volumes/ecommerce/source_data/raw/category/*.csv\"\n",
    "\n",
    "df_raw = spark.read.option(\"header\", \"true\").option(\"delimiter\", \",\").schema(category_schema).csv(raw_data_path)\n",
    "\n",
    "# Add metadata columns\n",
    "df_raw = df_raw.withColumn(\"_ingested_at\", F.current_timestamp()) \\\n",
    "               .withColumn(\"_source_file\", F.col(\"_metadata.file_path\"))\n",
    "\n",
    "\n",
    "# Write raw data to the Bronze layer (catalog: ecommerce, schema: bronze, table: brz_category)\n",
    "df_raw.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{catalog_name}.bronze.brz_category\")               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8966d532-9c41-468a-a303-219da07ad71d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "705e3fb3-1baa-48d4-8fc1-d43946efa5a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    FloatType,\n",
    "    IntegerType,\n",
    "    TimestampType\n",
    ")\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "products_schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), False),\n",
    "    StructField(\"sku\", StringType(), True),\n",
    "    StructField(\"category_code\", StringType(), True),\n",
    "    StructField(\"brand_code\", StringType(), True),\n",
    "    StructField(\"color\", StringType(), True),\n",
    "    StructField(\"size\", StringType(), True),\n",
    "    StructField(\"material\", StringType(), True),\n",
    "    StructField(\"weight_grams\", StringType(), True),  #datatype is string due to incoming data contain anamolies\n",
    "    StructField(\"length_cm\", StringType(), True),     #datatype is string due to incoming data contain anamolies\n",
    "    StructField(\"width_cm\", FloatType(), True),\n",
    "    StructField(\"height_cm\", FloatType(), True),\n",
    "    StructField(\"rating_count\", IntegerType(), True),\n",
    "    StructField(\"file_name\", StringType(), False),\n",
    "    StructField(\"ingest_timestamp\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "# Load data using the schema defined\n",
    "raw_data_path = \"/Volumes/ecommerce/source_data/raw/products/*.csv\"\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \",\").schema(products_schema).csv(raw_data_path) \\\n",
    "    .withColumn(\"file_name\", F.col(\"_metadata.file_path\")) \\\n",
    "    .withColumn(\"ingest_timestamp\", F.current_timestamp())\n",
    "\n",
    "# Write raw data to the Bronze layer (catalog: ecommerce, schema: bronze, table: brz_products)\n",
    "df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{catalog_name}.bronze.brz_products\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7719b98a-69e1-4bb6-850b-c2b76844374e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7113d1a-3d55-4cd9-9d79-37bcafcb8575",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"country_code\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load data using the schema defined\n",
    "raw_data_path =\"/Volumes/ecommerce/source_data/raw/customers/*.csv\"\n",
    "\n",
    "df_raw = spark.read.option(\"header\", \"true\").option(\"delimiter\", \",\").schema(customers_schema).csv(raw_data_path) \\\n",
    "    .withColumn(\"file_name\", F.col(\"_metadata.file_path\")) \\\n",
    "    .withColumn(\"ingest_timestamp\", F.current_timestamp())\n",
    "\n",
    "# Write raw data to the Bronze layer (catalog: ecommerce, schema: bronze, table: brz_customers)\n",
    "df_raw.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{catalog_name}.bronze.brz_customers\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0396c68-2c6a-4ebf-be9e-9404deb32493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab6e4335-8b0a-4b76-a4ea-58053b56482a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema for the data file\n",
    "date_schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),           # Raw date in string format\n",
    "    StructField(\"year\", IntegerType(), True),          # Year\n",
    "    StructField(\"day_name\", StringType(), True),       # Day name (can be mixed case)\n",
    "    StructField(\"quarter\", IntegerType(), True),       # Quarter\n",
    "    StructField(\"week_of_year\", IntegerType(), True),  # Week of year (can be negative)\n",
    "])\n",
    "\n",
    "# Load data using the schema defined\n",
    "raw_data_path = f\"/Volumes/ecommerce/source_data/raw/date/*.csv\" \n",
    "\n",
    "df_raw = spark.read.option(\"header\", \"true\").option(\"delimiter\", \",\").schema(date_schema).csv(raw_data_path)\n",
    "\n",
    "# Add metadata columns\n",
    "df_raw = df_raw.withColumn(\"_ingested_at\", F.current_timestamp()) \\\n",
    "               .withColumn(\"_source_file\", F.col(\"_metadata.file_path\"))\n",
    "\n",
    "\n",
    "# Write raw data to the Bronze layer (catalog: ecommerce, schema: bronze, table: brz_calendar) \n",
    "df_raw.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{catalog_name}.bronze.brz_calendar\")            "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1_dim_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
